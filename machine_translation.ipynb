{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Activation, LSTM, Dense, Bidirectional\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7576487632198033310\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15728113774827464196\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17265137480124615653\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3866165248\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15473157447889525017\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in english_sentences:\n",
    "    if re.findall(r'[A-Z]', sentence):\n",
    "        print(sentence)\n",
    "\n",
    "#All sentences are normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and create oe hot encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "    return np.array(tokenizer.texts_to_sequences(sentences)), tokenizer\n",
    "\n",
    "eng_tokenized_id, en_tokenizer = tokenize(english_sentences)\n",
    "fre_tokenized_id, fr_tokenizer = tokenize(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count no. of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_en_count = len(en_tokenizer.word_counts)\n",
    "unique_fr_count = len(fr_tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding all sentences to same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_padding(sentences, length=None):\n",
    "    return pad_sequences(sentences, maxlen=length, padding='post')\n",
    "\n",
    "eng_tokenized_id = apply_padding(eng_tokenized_id, 21)\n",
    "fre_tokenized_id = apply_padding(fre_tokenized_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_tokenized_id = np.reshape(eng_tokenized_id, newshape=(-1, eng_tokenized_id.shape[1], 1))\n",
    "fre_tokenized_id = np.reshape(fre_tokenized_id, newshape=(-1, fre_tokenized_id.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 21, 199)           39601     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 21, 512)           933888    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21, 16)            8208      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21, 344)           5848      \n",
      "=================================================================\n",
      "Total params: 987,545\n",
      "Trainable params: 987,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 103395 samples, validate on 34466 samples\n",
      "Epoch 1/10\n",
      "103395/103395 [==============================] - 61s 593us/step - loss: 1.1148 - acc: 0.7400 - val_loss: nan - val_acc: 0.8900\n",
      "Epoch 2/10\n",
      "103395/103395 [==============================] - 58s 561us/step - loss: 0.2922 - acc: 0.9186 - val_loss: nan - val_acc: 0.9369\n",
      "Epoch 3/10\n",
      "103395/103395 [==============================] - 58s 562us/step - loss: 0.1865 - acc: 0.9467 - val_loss: nan - val_acc: 0.9476\n",
      "Epoch 4/10\n",
      "103395/103395 [==============================] - 58s 564us/step - loss: 0.1408 - acc: 0.9594 - val_loss: nan - val_acc: 0.9606\n",
      "Epoch 5/10\n",
      "103395/103395 [==============================] - 58s 563us/step - loss: 0.1163 - acc: 0.9664 - val_loss: nan - val_acc: 0.9660\n",
      "Epoch 6/10\n",
      "103395/103395 [==============================] - 58s 565us/step - loss: 0.0996 - acc: 0.9710 - val_loss: nan - val_acc: 0.9694\n",
      "Epoch 7/10\n",
      "103395/103395 [==============================] - 58s 564us/step - loss: 0.0885 - acc: 0.9741 - val_loss: nan - val_acc: 0.9705\n",
      "Epoch 8/10\n",
      "103395/103395 [==============================] - 58s 565us/step - loss: 0.0792 - acc: 0.9766 - val_loss: nan - val_acc: 0.9733\n",
      "Epoch 9/10\n",
      "103395/103395 [==============================] - 58s 565us/step - loss: 0.0734 - acc: 0.9783 - val_loss: nan - val_acc: 0.9743\n",
      "Epoch 10/10\n",
      "103395/103395 [==============================] - 59s 567us/step - loss: 0.0668 - acc: 0.9800 - val_loss: nan - val_acc: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0b9ed7710>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(unique_en_count, unique_en_count, input_shape=eng_tokenized_id.shape[1:]))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True, activation='tanh')))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(unique_fr_count, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "model.fit(eng_tokenized_id, fre_tokenized_id, batch_size=64, epochs=10, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 21, 344)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(eng_tokenized_id[:1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating output to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking some outputs ----\n",
      "\n",
      "0/ English sentence -> \n",
      "   new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "   Actual translation ->\n",
      "   new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "   Predicted translation ->\n",
      "   new jersey est parfois calme pendant l' automne et il est neigeux avril avril       \n",
      "\n",
      "   Predicted word indexes ->\n",
      "   35 34 1 8 67 37 11 24 6 3 1 112 50 50 0 0 0 0 0 0 0 \n",
      "   Actual word indexes ->\n",
      "   35 34 1 8 67 37 11 24 6 3 1 112 2 50 0 0 0 0 0 0 0\n",
      "\n",
      "1/ English sentence -> \n",
      "   the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\n",
      "   Actual translation ->\n",
      "   les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "   Predicted translation ->\n",
      "   les états unis est généralement froid en juillet et il gèle habituellement en novembre       \n",
      "\n",
      "   Predicted word indexes ->\n",
      "   4 32 31 1 12 19 2 49 6 3 95 69 2 51 0 0 0 0 0 0 0 \n",
      "   Actual word indexes ->\n",
      "   4 32 31 1 12 19 2 49 6 3 95 69 2 51 0 0 0 0 0 0 0\n",
      "\n",
      "2/ English sentence -> \n",
      "   california is usually quiet during march , and it is usually hot in june .\n",
      "\n",
      "   Actual translation ->\n",
      "   california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n",
      "   Predicted translation ->\n",
      "   california est généralement calme en mars et il est généralement chaud en juin        \n",
      "\n",
      "   Predicted word indexes ->\n",
      "   101 1 12 67 2 45 6 3 1 12 21 2 41 0 0 0 0 0 0 0 0 \n",
      "   Actual word indexes ->\n",
      "   101 1 12 67 2 45 6 3 1 12 21 2 41 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "def id_to_text(logits=None):\n",
    "    index_to_words = {id: word for word, id in fr_tokenizer.word_index.items()}\n",
    "    index_to_words[0] = \"\"\n",
    "    for i in range(logits.shape[0]):\n",
    "        print(\"\\n\", i, \"/ English sentence -> \", sep=\"\")\n",
    "        print(\"  \", english_sentences[i])\n",
    "        print(\"\\n   Actual translation ->\")\n",
    "        print(\"  \", french_sentences[i])\n",
    "        print(\"\\n   Predicted translation ->\")\n",
    "        print(\"  \", \" \".join(index_to_words[prediction] for prediction in np.argmax(logits, 2)[i]))\n",
    "        \n",
    "        print(\"\\n   Predicted word indexes ->\")\n",
    "        print(\"   \", end=\"\")\n",
    "        for prediction in np.argmax(logits, 2)[i]:\n",
    "            print(prediction, end=\" \")\n",
    "        print(\"\\n   Actual word indexes ->\")\n",
    "        print(\"  \", \" \".join(np.reshape(fre_tokenized_id[i], newshape=(1, 21)).astype(str)[0]))\n",
    "\n",
    "print(\"Checking some outputs ----\")\n",
    "id_to_text(model.predict(eng_tokenized_id[0:3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
